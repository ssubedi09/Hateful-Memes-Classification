Hateful memes pose a significant challenge in content
moderation due to their multimodal nature. The sarcasm,
cultural context, or implicit meaning in memes frequently
arise from the interplay between the image and the caption.
This project explores the classification of hateful memes us
ing three different approaches: an image-based classifier,
a text-based classifier, and a multimodal CLIP-based classifier that integrates both image and text features through
different fusion strategies—concatenation, ensemble, and
self-attention—followed by a deep neural network classifier.
The primary objective is to maximize the detection of hateful memes while minimizing false positives on non-hateful
content.
