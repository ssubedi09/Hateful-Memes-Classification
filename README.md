Hateful memes pose a significant challenge in content moderation due to their multimodal nature. The sarcasm, cultural context, or implicit meaning in memes frequently arise from the interplay between the image and the caption. This project explores the classification of hateful memes using three different approaches: an image-based classifier,
a text-based classifier, and a multimodal CLIP-based classifier that integrates both image and text features through different fusion strategies—concatenation, ensemble, and self-attentio— followed by a deep neural network classifier. The primary objective is to maximize the detection of hateful memes while minimizing false positives on non-hateful content.

We use the dataset from Phase I of the Facebook Hateful Memes Challenge. The dataset is divided into three parts: a training set containing 8,500 memes (35% labeled as hateful and 65% as non-hateful), a validation set with 500 memes (50% hateful and 50% non-hateful), and a test set of 1,000 memes (50% hate-ful and 50% non-hateful).
